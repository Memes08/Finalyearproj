# -*- coding: utf-8 -*-
"""Untitled12.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PSDb2XARwyRaP4ANUuea4lfCSpBDQcTo
"""

# Install dependencies
!pip install --upgrade --quiet langchain langchain-community langchain-groq langchain-experimental openai-whisper neo4j

!pip install -q whisperx transformers neo4j langchain groq python-dotenv

!pip install -q git+https://github.com/langchain-ai/langchain.git

# Neo4j credentials
NEO4J_URI="neo4j+s://62889374.databases.neo4j.io"
NEO4J_USERNAME="neo4j"
NEO4J_PASSWORD="0chgvA5qcV_XPqRHJqDs3Q3Xf1dy-zHnWRGrHrOU38s"

import os
os.environ["NEO4J_URI"]=NEO4J_URI
os.environ["NEO4J_USERNAME"]=NEO4J_USERNAME
os.environ["NEO4J_PASSWORD"]=NEO4J_PASSWORD

from langchain_community.graphs import Neo4jGraph
graph = Neo4jGraph(
    url=NEO4J_URI,
    username=NEO4J_USERNAME,
    password=NEO4J_PASSWORD,
)

graph

# LLM + Prompt
from langchain_groq import ChatGroq
from langchain_core.prompts import PromptTemplate
from langchain_experimental.graph_transformers import LLMGraphTransformer

groq_api_key = "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"
llm = ChatGroq(groq_api_key=groq_api_key,temperature=0.2, model_name="Gemma2-9b-It")

from langchain_core.prompts import PromptTemplate

prompt = PromptTemplate.from_template("""
You are an expert in knowledge graph construction.

From the following CSV record, extract entity-relationship-entity triples that represent facts about the movie.

For each row in the input, extract:
- (Movie Title) -[HAS_ACTOR]-> (Actor)
- (Movie Title) -[HAS_DIRECTOR]-> (Director)
- (Movie Title) -[IN_GENRE]-> (Genre)
- (Movie Title) -[RELEASED_ON]-> (Release Date)
- (Movie Title) -[HAS_IMDB_RATING]-> (Rating)

Assume genres, actors, and directors are comma-separated if multiple.

Use this output format:
(Movie Title) -[RELATION]-> (Entity)
One triplet per line.

Do not invent data. Only use what's in the input.

Input:
{input}
""")


transformer = LLMGraphTransformer(llm=llm, prompt=prompt)

from google.colab import files
from moviepy.editor import VideoFileClip
import whisper
import urllib.request
import pandas as pd
import csv
from langchain_core.documents import Document

# Input choice
choice = ""
while choice not in {"1", "2", "3"}:
    print("Choose input method:\n1. Video upload\n2. CSV upload\n3. GitHub CSV URL")
    choice = input("Enter 1, 2, or 3: ").strip()

csv_path = "input_data.csv"

if choice == "1":
    print("Upload video...")
    uploaded = files.upload()
    video_file = next(iter(uploaded))
    video_path = f"/content/{video_file}"
    clip = VideoFileClip(video_path)
    clip.audio.write_audiofile("audio.wav")

    print("Transcribing...")
    model = whisper.load_model("tiny")
    result = model.transcribe("audio.wav")
    text = result["text"]
    print("\nðŸ“„ Transcription:\n", text)

    documents = [Document(page_content=text)]
    graph_documents = transformer.convert_to_graph_documents(documents)

    with open(csv_path, "w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["entity1", "relation", "entity2"])
        for rel in graph_documents[0].relationships:
            src = rel.source.properties.get("name", "UNKNOWN")
            tgt = rel.target.properties.get("name", "UNKNOWN")
            writer.writerow([src, rel.type, tgt])

elif choice == "2":
    print("Upload CSV file...")
    uploaded = files.upload()
    csv_file = next(iter(uploaded))
    csv_path = f"/content/{csv_file}"

elif choice == "3":
    github_url = input("Enter raw GitHub CSV URL: ")
    urllib.request.urlretrieve(github_url, csv_path)
    print(f"Downloaded to {csv_path}")

# Prepare input sentences
df = pd.read_csv(csv_path)
sentences = []
for _, row in df.iterrows():
    parts = [f"{col} is {val}" for col, val in row.items() if pd.notna(val)]
    sentences.append(", ".join(parts))

def split_chunks(sentences, max_tokens=300):
    chunks, chunk, tokens = [], [], 0
    for s in sentences:
        t = len(s.split()) * 1.5
        if tokens + t > max_tokens:
            chunks.append(chunk)
            chunk, tokens = [s], t
        else:
            chunk.append(s)
            tokens += t
    if chunk: chunks.append(chunk)
    return chunks

chunks = split_chunks(sentences)

import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def validate_relationships(relationships):
    for rel in relationships:
        if not hasattr(rel, 'source') or not hasattr(rel, 'target') or not hasattr(rel, 'type'):
            raise ValueError(f"Invalid relationship format: {rel}")
        if not hasattr(rel.source, 'properties') or not hasattr(rel.target, 'properties'):
            raise ValueError(f"Missing node properties: {rel}")
    return True

def process_chunk(chunk):
    try:
        docs = [Document(page_content=" ".join(chunk))]
        result = transformer.convert_to_graph_documents(docs)
        print(result)

        # Validate and print relationships
        for doc in result:
            validate_relationships(doc.relationships)
            for rel in doc.relationships:
                src = rel.source.properties.get("name", "UNKNOWN")
                tgt = rel.target.properties.get("name", "UNKNOWN")
                print(f"ðŸ”— {src} --[{rel.type}]--> {tgt}")

        return result
    except Exception as e:
        logging.error(f"âš ï¸ Error processing chunk: {e}")
        return []

# Process chunks
graph_documents = []
for i, chunk in enumerate(chunks):
    print(f"\nðŸ”¹ Chunk {i+1}/{len(chunks)}")
    result = process_chunk(chunk)
    graph_documents.extend(result)


# Save extracted triplets
with open("triplets.csv", "w", newline="") as f:
    writer = csv.writer(f)
    writer.writerow(["entity1", "relation", "entity2"])
    for doc in graph_documents:
        for rel in getattr(doc, "relationships", []):
            src = rel.source.properties.get("name", "UNKNOWN")
            tgt = rel.target.properties.get("name", "UNKNOWN")
            writer.writerow([src, rel.type, tgt])

# Insert into Neo4j
with open("triplets.csv", newline="") as f:
    reader = csv.DictReader(f)
    for row in reader:
        e1, rel, e2 = row["entity1"], row["relation"], row["entity2"]
        cypher = f'''
        MERGE (a:Entity {{name: "{e1}"}})
        MERGE (b:Entity {{name: "{e2}"}})
        MERGE (a)-[:{rel.upper().replace(" ", "_")}]->(b)
        '''
        graph.query(cypher)

graph.refresh_schema()
print("âœ… Graph updated!")

# Querying
from langchain.chains import GraphCypherQAChain
qa = GraphCypherQAChain.from_llm(llm=llm, graph=graph, verbose=True, allow_dangerous_requests=True)

query = input("ðŸ”Ž Ask a question: ")
response = qa.invoke({"query": query})
print("\nðŸ“£ Answer:\n", response["result"])

